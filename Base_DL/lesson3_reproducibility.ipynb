{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 3\n",
    "Это часть 2/2: эксперименты.\n",
    "## План (напоминание)\n",
    "В этой практической работе мы узнаем новые трюки и попробуем через эксперименты посмотреть, насколько эти трюки хороши.\n",
    "\n",
    "В первом ноутбуке мы знакомились с Dropout и Batch Normalization слоями.\n",
    "\n",
    "**Теперь же** мы вернемся к модели классификации изображений из прошлого урока и попробуем ее улучшать.\n",
    "А именно:\n",
    "- настроим сохранение метрик в wandb, обучим бейзлайн и сохраним его метрики;\n",
    "- добьемся воспроизводимости обучения;\n",
    "- попробуем применить LR Scheduler, сравним метрики;\n",
    "- попробуем добавить Dropout и Batch Normalization, сравним метрики;\n",
    "\n",
    "Помимо этого, мы разберем, как не потерять обученную модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты: ставим и сравниваем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подключаем wandb к пайплайну обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня вознакли сложности с работой wandb в облаке из-за санкуий США.\n",
    "wandb можно развернуть локально при наличии проблем с онлайн версией\n",
    "Нужно скачать docker image:\n",
    "> docker pull wandb/local\n",
    "\n",
    "Затем запустить сервер\n",
    "> wandb server start\n",
    "Команда поднимет контейнер с wandb/local и создаст volume для хранения данных\n",
    "\n",
    "Для остановки сервера:\n",
    "> wandb server stop\n",
    "\n",
    "Нужно сгенерировать apikey на локальном сервере\n",
    "\n",
    "Можно на deploy.wandb.ai сгенерировать бесплатную лицензию и скопировать ее в localhost:8080/system-admin, чтобы не было предупреждения на главном экране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# with open(\"../wandb_apikey\") as apikey:\n",
    "#     key = apikey.read()\n",
    "\n",
    "# print(key)\n",
    "# wandb.login(key=key,\n",
    "#             host=\"http://localhost:8080\",\n",
    "#             relogin=True)\n",
    "run = wandb.init(project=\"project-1\")  # БЕЗ entity!\n",
    "run.log({\"metric\": 1101})\n",
    "run.finish()\n",
    "\n",
    "print(\"✅ Работает!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нужно установить библиотеку wandb\n",
    "import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "# затем можно запускать любой код и логгировать любые метрики\n",
    "\n",
    "def simple_train_loop():\n",
    "    # Сначала нужно вызвать wandb.init()\n",
    "    # Это создаст эксперимент в wandb.ai и привяжет его к текущему запуску.\n",
    "    run = wandb.init(\n",
    "        # Более детальное описание аргументов: https://docs.wandb.ai/guides/track/launch\n",
    "        project=\"wandb-project\",\n",
    "        notes=\"I created it in my DL course\",\n",
    "        # Можно так же передать config - словарь с любым содержимым.\n",
    "        # Обычно туда кладут гиперпараметры, настройки обработки данных, random seed и т.д.\n",
    "        config={\"seed\": 0, \"my-custom_string\": \"asb\"},\n",
    "    )\n",
    "    # Теперь запускаем наш код обучения, подготовки данных и т.п. как обычно\n",
    "    for i in tqdm.trange(300):\n",
    "        # Имитируем долгое обучение\n",
    "        time.sleep(0.01)\n",
    "        # Нужно добавить эту строку, чтобы записать в wandb\n",
    "        run.log({\"iteration\": i, \"loss\": 10 - i ** 0.3})\n",
    "    # Запуск автоматически завершится, когда скрипт (т.е. ноутбук) завершит работу.\n",
    "    # Но можно явно завершить:\n",
    "    run.finish()\n",
    "\n",
    "# запустим, смотрим\n",
    "simple_train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Готовим пайплайн классификации\n",
    "\n",
    "Достанем код из прошлого урока, построим модель, добавим в обучение wandb - и убедимся, что все работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь делаем все серьезно.\n",
    "# Загрузим данные, обучим модель, отрисуем в wandb графики\n",
    "# Дальше идет код с предыдущей лекции\n",
    "import http.client\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Скачивает данные и распаковывает их.\"\"\"\n",
    "    target_file = \"notMNIST_small.tar.gz\"\n",
    "    if Path(target_file).exists():\n",
    "        print(\"Файл уже загружен, не загружаю снова\")\n",
    "    else:\n",
    "        conn = http.client.HTTPConnection(\"yaroslavvb.com\", 80)\n",
    "        conn.request(\"GET\", \"/upload/notMNIST/notMNIST_small.tar.gz\")\n",
    "        data = conn.getresponse().read()\n",
    "        with open(target_file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "    with tarfile.open(target_file) as f:\n",
    "        f.extractall(filter=\"data\")\n",
    "    print(\"Данные были скачены и распакованы\")\n",
    "\n",
    "\n",
    "def read_notmnist_data(\n",
    "    data_dir: str = \"notMNIST_small\",\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Прочитать картинки датасета notMNIST и положить их в numpy-массив.\n",
    "\n",
    "    :returns: пару numpy-массивов (изображения, соответствующие метки)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "    for img_path in Path(data_dir).glob(\"**/*.png\"):\n",
    "        # Имя папки - это метка класса\n",
    "        img_label = img_path.parts[1]\n",
    "        try:\n",
    "            image = plt.imread(img_path)\n",
    "        except SyntaxError:\n",
    "            print(\n",
    "                f\"Изображение не читается по пути {img_path} (это ок, но таких должно быть < 10)\"\n",
    "            )\n",
    "            continue\n",
    "        labels.append(img_label)\n",
    "        images.append(image)\n",
    "    return np.stack(images, axis=0), np.stack(labels, axis=0)\n",
    "\n",
    "\n",
    "prepare_data()\n",
    "X, y = read_notmnist_data()\n",
    "assert X.shape[0] == y.shape[0]\n",
    "ohe = LabelEncoder()\n",
    "y = ohe.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seed = 0\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=1\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=1\n",
    ")\n",
    "X_train, y_train = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "X_val, y_val = torch.from_numpy(X_val), torch.from_numpy(y_val)\n",
    "X_test, y_test = torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        idx = 4 * row + col\n",
    "        ax[row][col].imshow(X_train[idx])\n",
    "        ax[row][col].set_title(f\"label={y_train[idx]}\")\n",
    "# конец кода с предыдущей лекции\n",
    "# Перезапустите эту ячейку несколько раз - изображения будут меняться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = 256\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=28 * 28, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=num_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.sgd import SGD\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# конфиги можно сделать в словаре, а можно в датаклассе - будут подсказки в редакторе\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    eval_every: int = 10\n",
    "    lr: float = 1e-2\n",
    "    total_iterations: int = 3000\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    config: TrainConfig,\n",
    "):\n",
    "    wandb.init(\n",
    "        project=\"simple-model-train\",\n",
    "        notes=\"version 1\",\n",
    "        tags=[\"sgd\", \"2-layer\"],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    optim = SGD(model.parameters(), lr=config.lr)\n",
    "    model.train()\n",
    "    for i in tqdm.trange(config.total_iterations):\n",
    "        optim.zero_grad()\n",
    "        loss = F.cross_entropy(model(X_train), y_train)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        metrics = {\"iteration\": i, \"loss_train\": loss.detach().cpu().item()}\n",
    "        # каждые `eval_every` итераций будем считать метрику на отложенной выборке\n",
    "        if (i + 1) % config.eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_val = F.cross_entropy(model(X_val), y_val)\n",
    "                model.train()\n",
    "                metrics.update({\"loss_val\": loss_val.cpu().item()})\n",
    "        wandb.log(metrics)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "config = TrainConfig(eval_every=20, lr=1e-1, total_iterations=500)\n",
    "model = SimpleModel(num_classes=len(ohe.classes_))\n",
    "train_loop(model, X_train, y_train, X_val, y_val, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем то же самое на GPU\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Используем\", device)\n",
    "\n",
    "def train_loop_device(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    config: TrainConfig,\n",
    "):\n",
    "    wandb.init(\n",
    "        project=\"simple-model-train\",\n",
    "        notes=\"version 1\",\n",
    "        tags=[\"sgd\", \"2-layer\"],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    X_val_dev = X_val.clone().to(device)\n",
    "    y_val_dev = y_val.clone().to(device)\n",
    "\n",
    "    optim = SGD(model.parameters(), lr=config.lr)\n",
    "    # Перенесем на GPU если возможно\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for i in tqdm.trange(config.total_iterations):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        X_train_dev = X_train.clone().to(device)\n",
    "        y_train_dev = y_train.clone().to(device)\n",
    "\n",
    "        loss = F.cross_entropy(model(X_train_dev), y_train_dev)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        metrics = {\"iteration\": i, \"loss_train\": loss.detach().cpu().item()}\n",
    "        # каждые `eval_every` итераций будем считать метрику на отложенной выборке\n",
    "        if (i + 1) % config.eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_val = F.cross_entropy(model(X_val_dev), y_val_dev)\n",
    "                model.train()\n",
    "                metrics.update({\"loss_val\": loss_val.cpu().item()})\n",
    "        wandb.log(metrics)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "config = TrainConfig(eval_every=20, lr=1e-1, total_iterations=500)\n",
    "model = SimpleModel(num_classes=len(ohe.classes_))\n",
    "train_loop_device(model, X_train, y_train, X_val, y_val, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если перезапустить ноутбук, то графики получатся другие.\n",
    "\n",
    "Но как же так, мы ничего не меняли в коде?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добиваемся воспроизводимости\n",
    "Пройдемся по ноутбуку и найдем все места, где есть случайности:\n",
    "- разбиение на train/val/test - это видно по `shuffle=True`;\n",
    "- инициализация весов модели - веса всех слоев генерируются из случайного распределения;\n",
    "- могут быть алгоритмы внутри слоев, но у нас таких нету.\n",
    "\n",
    "Чтобы добиться воспроизводимости, нужно:\n",
    "1. Добавить `random_state=...` в `train_test_split` - он умеет принимать такой параметр.\n",
    "2. Зафиксировать `seed` у PyTorch **перед** созданием модели через `torch.random.manual_seed`.\n",
    "\n",
    "Идем и исправляем это, после чего перезапускаем ячейки.\n",
    "Теперь графики перестали меняться от запуска к запуску."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пробуем новые подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем LR Scheduler и Adam\n",
    "Добавим шедулер, его настройки в конфиг, а также переберем несколько вариантов.\n",
    "\n",
    "Аналогично сделаем для оптимайзера: будем пробовать Adam и SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, LinearLR, StepLR\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "\n",
    "# конфиги можно сделать в словаре, а можно в датаклассе - будут подсказки в редакторе\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    eval_every: int = 10\n",
    "    lr: float = 1e-2\n",
    "    total_iterations: int = 3000\n",
    "    scheduler_type: tp.Literal[\"exp\", \"linear\", \"step\"] | None = None\n",
    "    optimizer_type: tp.Literal[\"sgd\", \"adam\"] = \"sgd\"\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    config: TrainConfig,\n",
    "    run_name: str | None = None,\n",
    "):\n",
    "    wandb.init(\n",
    "        project=\"simple-model-train\",\n",
    "        notes=\"version 1\",\n",
    "        # еще добавим возможность называть запуски\n",
    "        name=run_name,\n",
    "        tags=[config.optimizer_type, str(config.scheduler_type)],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    #### Новое: создаем разный optimizer в зависимости от конфига ####\n",
    "    if config.optimizer_type == \"sgd\":\n",
    "        optim = SGD(model.parameters(), lr=config.lr)\n",
    "    else:\n",
    "        optim = Adam(model.parameters(), lr=config.lr)\n",
    "    #### Новое: создаем шедулер ####\n",
    "    # LR scheduler на вход принимает optimizer и некоторые параметры (которые зависят от его алгоритма)\n",
    "    if config.scheduler_type == \"exp\":\n",
    "        scheduler = ExponentialLR(optim, gamma=0.99)\n",
    "    elif config.scheduler_type == \"linear\":\n",
    "        scheduler = LinearLR(\n",
    "            optim, start_factor=1.0, end_factor=0.1, total_iters=config.total_iterations\n",
    "        )\n",
    "    elif config.scheduler_type == \"step\":\n",
    "        scheduler = StepLR(optim, step_size=10, gamma=0.9)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    #####\n",
    "\n",
    "    model.train()\n",
    "    for i in tqdm.trange(config.total_iterations):\n",
    "        optim.zero_grad()\n",
    "        loss = F.cross_entropy(model(X_train), y_train)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        metrics = {\"iteration\": i, \"loss_train\": loss.detach().cpu().item()}\n",
    "        # каждые `eval_every` итераций будем считать метрику на отложенной выборке\n",
    "        if (i + 1) % config.eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_val = F.cross_entropy(model(X_val), y_val)\n",
    "                model.train()\n",
    "                metrics.update({\"loss_val\": loss_val.cpu().item()})\n",
    "        if scheduler is not None:\n",
    "            # для scheduler точно так же надо звать .step(), но после обучения и валидации\n",
    "            # см. пример в документации: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
    "            scheduler.step()\n",
    "            metrics.update({\"lr\": scheduler.get_last_lr()[0]})\n",
    "        else:\n",
    "            # Чтобы иметь одинаковый набор графиков\n",
    "            metrics.update({\"lr\": config.lr})\n",
    "        wandb.log(metrics)\n",
    "    wandb.finish()\n",
    "    return optim\n",
    "\n",
    "\n",
    "# на exp быстрее к нулю сойдемся, без scheduler тогда ок.\n",
    "for optim in (\"sgd\", \"adam\"):\n",
    "    for scheduler_type in (None, \"exp\", \"linear\", \"step\"):\n",
    "        torch.random.manual_seed(seed)\n",
    "        config = TrainConfig(\n",
    "            eval_every=20,\n",
    "            lr=2,\n",
    "            total_iterations=500,\n",
    "            scheduler_type=scheduler_type,\n",
    "            optimizer_type=optim,\n",
    "        )\n",
    "        model = SimpleModel(num_classes=len(ohe.classes_))\n",
    "        train_loop(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            config=config,\n",
    "            run_name=f\"optim={optim}__lr_sched={scheduler_type}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем то же самое на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, LinearLR, StepLR\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Используем\", device)\n",
    "\n",
    "# конфиги можно сделать в словаре, а можно в датаклассе - будут подсказки в редакторе\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    eval_every: int = 10\n",
    "    lr: float = 1e-2\n",
    "    total_iterations: int = 3000\n",
    "    scheduler_type: tp.Literal[\"exp\", \"linear\", \"step\"] | None = None\n",
    "    optimizer_type: tp.Literal[\"sgd\", \"adam\"] = \"sgd\"\n",
    "\n",
    "\n",
    "def train_loop_dev(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    config: TrainConfig,\n",
    "    run_name: str | None = None,\n",
    "):\n",
    "    wandb.init(\n",
    "        project=\"simple-model-train\",\n",
    "        notes=\"version 1\",\n",
    "        # еще добавим возможность называть запуски\n",
    "        name=run_name,\n",
    "        tags=[config.optimizer_type, str(config.scheduler_type)],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    X_val_dev = X_val.clone().to(device)\n",
    "    y_val_dev = y_val.clone().to(device)\n",
    "\n",
    "    #### Новое: создаем разный optimizer в зависимости от конфига ####\n",
    "    if config.optimizer_type == \"sgd\":\n",
    "        optim = SGD(model.parameters(), lr=config.lr)\n",
    "    else:\n",
    "        optim = Adam(model.parameters(), lr=config.lr)\n",
    "    #### Новое: создаем шедулер ####\n",
    "    # LR scheduler на вход принимает optimizer и некоторые параметры (которые зависят от его алгоритма)\n",
    "    if config.scheduler_type == \"exp\":\n",
    "        scheduler = ExponentialLR(optim, gamma=0.99)\n",
    "    elif config.scheduler_type == \"linear\":\n",
    "        scheduler = LinearLR(\n",
    "            optim, start_factor=1.0, end_factor=0.1, total_iters=config.total_iterations\n",
    "        )\n",
    "    elif config.scheduler_type == \"step\":\n",
    "        scheduler = StepLR(optim, step_size=10, gamma=0.9)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    #####\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for i in tqdm.trange(config.total_iterations):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        X_train_dev = X_train.clone().to(device)\n",
    "        y_train_dev = y_train.clone().to(device)      \n",
    "        \n",
    "        loss = F.cross_entropy(model(X_train_dev), y_train_dev)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        metrics = {\"iteration\": i, \"loss_train\": loss.detach().cpu().item()}\n",
    "        # каждые `eval_every` итераций будем считать метрику на отложенной выборке\n",
    "        if (i + 1) % config.eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_val = F.cross_entropy(model(X_val_dev), y_val_dev)\n",
    "                model.train()\n",
    "                metrics.update({\"loss_val\": loss_val.cpu().item()})\n",
    "        if scheduler is not None:\n",
    "            # для scheduler точно так же надо звать .step(), но после обучения и валидации\n",
    "            # см. пример в документации: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
    "            scheduler.step()\n",
    "            metrics.update({\"lr\": scheduler.get_last_lr()[0]})\n",
    "        else:\n",
    "            # Чтобы иметь одинаковый набор графиков\n",
    "            metrics.update({\"lr\": config.lr})\n",
    "        wandb.log(metrics)\n",
    "    wandb.finish()\n",
    "    return optim\n",
    "\n",
    "\n",
    "# на exp быстрее к нулю сойдемся, без scheduler тогда ок.\n",
    "for optim in (\"sgd\", \"adam\"):\n",
    "    for scheduler_type in (None, \"exp\", \"linear\", \"step\"):\n",
    "        torch.random.manual_seed(seed)\n",
    "        config = TrainConfig(\n",
    "            eval_every=20,\n",
    "            lr=2,\n",
    "            total_iterations=500,\n",
    "            scheduler_type=scheduler_type,\n",
    "            optimizer_type=optim,\n",
    "        )\n",
    "        model = SimpleModel(num_classes=len(ohe.classes_))\n",
    "        train_loop_dev(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            config=config,\n",
    "            run_name=f\"optim={optim}__lr_sched={scheduler_type}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Что изменил LR Scheduler\n",
    "Добавление экспоненциального шедулера помогло в начале, но в конце привело к худшей метрике,\n",
    "чем полное отсутствие шедулера.\n",
    "\n",
    "Делаем вывод, что LR scheduler не всегда дает пользу.\n",
    "\n",
    "В больших сетях без него не обойтись, поскольку большие нейросети бывают капризными к выбору learning rate.\n",
    "Наша же сеть простая, поэтому для нее не имеет разницы - что есть шедулер, что его нет.\n",
    "\n",
    "#### Что изменил другой оптимайзер\n",
    "Смена SGD на Adam привела к ухудшению результата модели.\n",
    "Делаем вывод, что менять оптимайзер - не самая лучшая идея, если хотим выбить больше качества.\n",
    "\n",
    "Но если модель не хочет учиться, то смена оптимайзера может спасти дело.\n",
    "Такое случается в больших моделях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем Dropout и Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self, num_classes: int, p_dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        hidden_dim = 256\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=28 * 28, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=num_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "config = TrainConfig(\n",
    "    eval_every=20,\n",
    "    lr=2,\n",
    "    total_iterations=500,\n",
    "    # возьмем None как самый лучший вариант\n",
    "    scheduler_type=None,\n",
    "    optimizer_type='sgd',\n",
    ")\n",
    "model = DropoutModel(num_classes=len(ohe.classes_))\n",
    "train_loop(model, X_train, y_train, X_val, y_val, config=config, run_name=\"add-dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лосс на трейне оказался чуть больше, но зато на валидации ошибка оказалась чуть-чуть получше.\n",
    "Это ожидаемо: dropout нацелен на то, чтобы бороться с переобучением.\n",
    "Dropout ставит палки в колеса модели, отсюда и увеличенный лосс на трейне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormModel(nn.Module):\n",
    "    def __init__(self, num_classes: int, p_dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        hidden_dim = 256\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=28 * 28, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=num_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "config = TrainConfig(\n",
    "    eval_every=20,\n",
    "    lr=2,\n",
    "    total_iterations=500,\n",
    "    scheduler_type=None,\n",
    "    optimizer_type=\"sgd\",\n",
    ")\n",
    "model = BatchNormModel(num_classes=len(ohe.classes_))\n",
    "optim = train_loop(\n",
    "    model, X_train, y_train, X_val, y_val, config=config, run_name=\"add-batchnorm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лосс упал и на трейне, и на валидации.\n",
    "\n",
    "Получается, нормализация выходов слоев действительно помогает выбивать лучшие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как сохранить обученную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, у нас есть улучшение бейзлайна - это SGD + BatchNormalization.\n",
    "\n",
    "Мы даже его уже обучили!\n",
    "Можем ли мы как-то сделать так, чтобы при следующем запуске не надо было заново учить? Да, можем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все состояние модели хранится в .state_dict() - словаре из тензоров\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это состояние можно сохранить на диск через torch.save\n",
    "\n",
    "# Обычно файлы pytorch сохраняют с расширением .pt (это не жесткое правило, скорее для понимания)\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# и потом можно загрузить\n",
    "\n",
    "# сохраняются веса, но не объект модели - поэтому его надо создать заново\n",
    "model_loaded = DropoutModel(num_classes=len(ohe.classes_))\n",
    "# грузим файл, затем просим pytorch восстановить state_dict из заданного словаря\n",
    "model_loaded.load_state_dict(torch.load(\"model.pt\"))\n",
    "# о нет, не получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если какие-то ключи не нашлись в файле или лишние, pytorch не даст загрузить.\n",
    "# Смотрим выше и понимаем, что модель была BatchNormModel - там другой слой.\n",
    "model_loaded = BatchNormModel(num_classes=len(ohe.classes_))\n",
    "model_loaded.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь все ок.\n",
    "# Проверим, что веса прогрузились те же, что и были\n",
    "from torch.testing import assert_close\n",
    "\n",
    "assert_close(model_loaded(X_train), model(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пара слов про optimizer\n",
    "Когда мы учим модель, оптимизатор тоже может хранить какие-то тензоры.\n",
    "\n",
    "Например, Adam хранит скользящее среднее и дисперсию по всем увиденным данным.\n",
    "\n",
    "Поэтому вместе с моделью надо сохранять и оптимизатор. Делается это точно так же, как с моделями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model_loaded.parameters())\n",
    "optim.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(optim.state_dict(), \"optimizer.pt\")\n",
    "optim_loaded = Adam(model.parameters())\n",
    "optim_loaded.load_state_dict(torch.load('optimizer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что будет плохого, если забыть про это и инициализировать с нуля оптимизатор?\n",
    "\n",
    "У вас не прогрузится то состояние, в котором был оптимизатор на момент последней итерации обучения.\n",
    "Соответственно, результаты получатся разными, если:\n",
    "- обучиться 100 итераций, сохранить на диск, потом продолжить и еще 100 итераций сделать;\n",
    "- сразу сделать 200 итераций;\n",
    "\n",
    "Это нехорошо, ведь мы хотим получать одинаковые результаты в обоих случаях.\n",
    "Поэтому **не забывайте сохранять состояние оптимизатора** вместе с весами модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "1. Посмотрели на работу Batch Normalization и Dropout в PyTorch.\n",
    "2. Познакомились с wandb и тем, как с его помощью логгировать метрики и графики.\n",
    "3. Узнали, как добиваться воспроизводимости на практике.\n",
    "4. Познакомились в LR Scheduler, попробовали его в эксперименте.\n",
    "5. Попробовали Batch Normalization и Dropout в имеющейся сети, сравнили качество.\n",
    "6. Лучшую модель сохранили на диск."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
