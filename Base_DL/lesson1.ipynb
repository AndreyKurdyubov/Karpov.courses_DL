{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Знакомство с инструментом PyTorch\n",
    "\n",
    "## План\n",
    "В этом ноутбуке посмотрим на базовые возможности PyTorch:\n",
    "\n",
    "0. Подсказки при работе с Jupyter Notebook.\n",
    "1. Как создать тензор.\n",
    "2. Операции с тензором.\n",
    "3. Функции над тензором.\n",
    "4. Градиенты в PyTorch.\n",
    "5. Функции потерь.\n",
    "6. Слои нейросети.\n",
    "8. PyTorch и видеокарта.\n",
    "\n",
    "## Почему именно PyTorch?\n",
    "Этот инструмент стал популярен в мире DL.\n",
    "Причин много, но из самых интересных стоит выделить три:\n",
    "1. PyTorch имеет numpy-подобный интерфейс, поэтому на него легко перейти после numpy.\n",
    "2. PyTorch умеет автоматически считать градиенты всех вычислений, независимо от количества операций.\n",
    "Можно учить модели произвольного размера.\n",
    "3. В PyTorch уже реализовано много часто используемых в DL операций и слоев нейросетей.\n",
    "4. Все вычисления на PyTorch без головной боли можно перенести на GPU и получить прирост x100 в скорости.\n",
    "\n",
    "## PyTorch: начало\n",
    "\n",
    "Если вы используете Google Colab или Kaggle Notebooks,\n",
    "то у вас уже установлен `pytorch`.\n",
    "На 2024-02-10 они оба используют версии 2.1.\n",
    "\n",
    "<details>\n",
    "<summary>**Если используете личный ноутбук или сервер**</summary>\n",
    "\n",
    "Ваш ноутбук или сервер должны иметь видеокарту, и `pytorch` должен \"увидеть\" ее.\n",
    "\n",
    "Сначала устанавливаем пакет `pytorch`.\n",
    "Лучше всего это делать в виртуальном окружении (можно и в anaconda).\n",
    "Выполняем в терминале команду:\n",
    "\n",
    "```bash\n",
    "pip install torch\n",
    "```\n",
    "\n",
    "Затем открываем интерпретатор Python и выполняем:\n",
    "```python\n",
    "import torch\n",
    "torch.cuda.is_avaliable()\n",
    "# Должно выдать True\n",
    "```\n",
    "\n",
    "Если выдало `True`, то `pytorch` увидел вашу видеокарту и может работать с ней.\n",
    "Если выдало `False` или ошибку, то рекомендуем прочитать [официальную инструкцию](https://pytorch.org/get-started/locally/) по установке - в ней описано, как установить `pytorch` так, чтобы он \"видел\" видеокарту.\n",
    "Если же и инструкция не помогла, то советуем работать в Google Colab или Kaggle Notebooks.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсказки при работе с Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:08:40.837501Z",
     "iopub.status.busy": "2026-02-04T05:08:40.837311Z",
     "iopub.status.idle": "2026-02-04T05:08:46.622219Z",
     "shell.execute_reply": "2026-02-04T05:08:46.621351Z",
     "shell.execute_reply.started": "2026-02-04T05:08:40.837481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d76d94cff10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:08:52.367359Z",
     "iopub.status.busy": "2026-02-04T05:08:52.366577Z",
     "iopub.status.idle": "2026-02-04T05:08:52.372190Z",
     "shell.execute_reply": "2026-02-04T05:08:52.371360Z",
     "shell.execute_reply.started": "2026-02-04T05:08:52.367328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.9.0+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:08:54.237140Z",
     "iopub.status.busy": "2026-02-04T05:08:54.236546Z",
     "iopub.status.idle": "2026-02-04T05:08:54.288003Z",
     "shell.execute_reply": "2026-02-04T05:08:54.287209Z",
     "shell.execute_reply.started": "2026-02-04T05:08:54.237102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sqrt  # Нажмите <Tab>, чтобы увидеть подсказки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m torch.Tensor(*args, **kwargs)\n",
      "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.12/dist-packages/torch/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m           _TensorMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     SparseSemiStructuredTensor, Parameter, Buffer, UninitializedBuffer, NestedTensor, FakeTensor, FunctionalTensor, LoggingTensor, MaskedTensor\n"
     ]
    }
   ],
   "source": [
    "# В Jupyter Notebook можно распечатать документацию к классу или функции\n",
    "# Для этого нужно написать в конце \"?\"\n",
    "torch.Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m torch.nn.functional.mse_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', weight: Optional[torch.Tensor] = None) -> torch.Tensor\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "def mse_loss(\n",
      "    input: Tensor,\n",
      "    target: Tensor,\n",
      "    size_average: Optional[bool] = None,\n",
      "    reduce: Optional[bool] = None,\n",
      "    reduction: str = \"mean\",\n",
      "    weight: Optional[Tensor] = None,\n",
      ") -> Tensor:\n",
      "    r\"\"\"Compute the element-wise mean squared error, with optional weighting.\n",
      "\n",
      "    See :class:`~torch.nn.MSELoss` for details.\n",
      "\n",
      "    Args:\n",
      "        input (Tensor): Predicted values.\n",
      "        target (Tensor): Ground truth values.\n",
      "        size_average (bool, optional): Deprecated (see :attr:`reduction`).\n",
      "        reduce (bool, optional): Deprecated (see :attr:`reduction`).\n",
      "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
      "                                   'none' | 'mean' | 'sum'. 'mean': the mean of the output is taken.\n",
      "                                   'sum': the output will be summed. 'none': no reduction will be applied.\n",
      "                                   Default: 'mean'.\n",
      "        weight (Tensor, optional): Weights for each sample. Default: None.\n",
      "\n",
      "    Returns:\n",
      "        Tensor: Mean Squared Error loss (optionally weighted).\n",
      "    \"\"\"\n",
      "    if has_torch_function_variadic(input, target, weight):\n",
      "        return handle_torch_function(\n",
      "            mse_loss,\n",
      "            (input, target, weight),\n",
      "            input,\n",
      "            target,\n",
      "            size_average=size_average,\n",
      "            reduce=reduce,\n",
      "            reduction=reduction,\n",
      "            weight=weight,\n",
      "        )\n",
      "\n",
      "    if not (target.size() == input.size()):\n",
      "        warnings.warn(\n",
      "            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n",
      "            \"This will likely lead to incorrect results due to broadcasting. \"\n",
      "            \"Please ensure they have the same size.\",\n",
      "            stacklevel=2,\n",
      "        )\n",
      "\n",
      "    if size_average is not None or reduce is not None:\n",
      "        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "\n",
      "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "\n",
      "    if weight is not None:\n",
      "        if weight.size() != input.size():\n",
      "            raise ValueError(\"Weights and input must have the same size.\")\n",
      "\n",
      "        # Perform weighted MSE loss manually\n",
      "        squared_errors = torch.pow(expanded_input - expanded_target, 2)\n",
      "        weighted_squared_errors = squared_errors * weight\n",
      "\n",
      "        if reduction == \"none\":\n",
      "            return weighted_squared_errors\n",
      "        elif reduction == \"sum\":\n",
      "            return torch.sum(weighted_squared_errors)\n",
      "        elif reduction == \"mean\":\n",
      "            return torch.sum(weighted_squared_errors) / torch.sum(weight)\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                f\"Invalid reduction mode: {reduction}. Expected one of 'none', 'mean', 'sum'.\"\n",
      "            )\n",
      "    else:\n",
      "        return torch._C._nn.mse_loss(\n",
      "            expanded_input, expanded_target, _Reduction.get_enum(reduction)\n",
      "        )\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "# Можно также распечатать весь исходный код, дописав в конец \"??\"\n",
    "torch.nn.functional.mse_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тензор: великий и ужасный\n",
    "\n",
    "Напомним: тензор — это всего лишь набор чисел, расфасованных по осям.\n",
    "О тензоре можно думать как о матрице — вот только если матрица была двумерна, то тензор может иметь три и более размерности.\n",
    "\n",
    "Тензор с размерностью 1 — это вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 — это матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше — это тензор, то есть список списков списков (и т.д.) чисел.\n",
    "\n",
    "\n",
    "![meme](./meme.png)\n",
    "\n",
    "#### Как создать тензор\n",
    "Научимся создавать:\n",
    "1. Тензор с непредсказуемыми данными (самый простой вариант).\n",
    "2. Тензор из нулей.\n",
    "3. Тензор, заполненный одним и тем же числом.\n",
    "4. Тензор со значениями из нормального распределения.\n",
    "\n",
    "Также познакомимся с in-place операциями и тем, как с ними не запутаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5766e-19, 1.0256e-08, 1.0255e-08, 3.0784e+12],\n",
       "         [4.3354e-08, 2.6260e-06, 1.7471e-04, 1.2610e+16],\n",
       "         [2.1707e-18, 7.0952e+22, 1.7748e+28, 1.8176e+31]],\n",
       "\n",
       "        [[7.2708e+31, 5.0778e+31, 3.2608e-12, 1.7728e+28],\n",
       "         [7.0367e+22, 2.1715e-18, 1.3108e-08, 2.7179e+23],\n",
       "         [8.4272e-07, 1.7183e-04, 1.6520e-04, 1.3040e-11]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Есть много способов создать тензор в torch.\n",
    "Посмотрим на некоторые из них.\n",
    "\"\"\"\n",
    "\n",
    "# Самый простой — запросить тензор определенной размерности\n",
    "\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "# Будет заполнен произвольными непредсказуемыми данными\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Можно спросить, какой размер. Помним, что было (2, 3, 4)\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Есть .shape — работает аналогично\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape можно удобно сравнивать с tuple — пригодится в тестах\n",
    "assert t.shape == (2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В torch много функций для самых \"ходовых\" тензоров.\n",
    "# Например, создать тензор из нулей\n",
    "# Сделаем матрицу (5, 3), заполненную нулями\n",
    "torch.zeros((5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]], dtype=torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тензор (2, 3, 4), все числа равны 1.\n",
    "# Обратите внимание на точку после 1. — это значит, что тип float\n",
    "t = torch.ones((2, 3, 4), dtype=torch.float16)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тензор (2, 3, 4), все числа равны 1.\n",
    "# Обратите внимание на точку после 1. — это значит, что тип float\n",
    "t = torch.ones((2, 3, 4))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# А если точнее — float32.\n",
    "# На лекции мы знакомились с fp16, fp32 — это оно и есть.\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.7000, 2.7000, 2.7000, 2.7000],\n",
       "         [2.7000, 2.7000, 2.7000, 2.7000]],\n",
       "\n",
       "        [[2.7000, 2.7000, 2.7000, 2.7000],\n",
       "         [2.7000, 2.7000, 2.7000, 2.7000]],\n",
       "\n",
       "        [[2.7000, 2.7000, 2.7000, 2.7000],\n",
       "         [2.7000, 2.7000, 2.7000, 2.7000]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# аналогично можно заполнить любыми числами:\n",
    "2.7 * torch.ones((3, 2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на квадратные скобки.\n",
    "По ним видно, что тензор как будто состоит из двух матриц 3x4, соединенных вместе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
       "          [ 0.6784, -1.2345, -0.0431, -1.6047]],\n",
       "\n",
       "         [[-0.7521,  1.6487, -0.3925, -1.4036],\n",
       "          [-0.7279, -0.5594, -0.7688,  0.7624]],\n",
       "\n",
       "         [[ 1.6423, -0.1596, -0.4974,  0.4396],\n",
       "          [-0.7581,  1.0783,  0.8008,  1.6806]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2791,  1.2964,  0.6105,  1.3347],\n",
       "          [-0.2316,  0.0418, -0.2516,  0.8599]],\n",
       "\n",
       "         [[-1.3847, -0.8712, -0.2234,  1.7174],\n",
       "          [ 0.3189, -0.4245,  0.3057, -0.7746]],\n",
       "\n",
       "         [[-1.5576,  0.9956, -0.8798, -0.6011],\n",
       "          [-1.2742,  2.1228, -1.2347, -0.4879]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тензор (2, 3, 2, 4), каждый элемент взят из стандартного нормального распределения\n",
    "torch.randn((2, 3, 2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### in-place операции\n",
    "Все рассмотренные выше операции создают **новый** тензор.\n",
    "Но иногда хочется не создавать новый, а менять существующий.\n",
    "\n",
    "Для этого есть т.н. \"in-place\" (\"на месте\") операции — они меняют тот тензор,\n",
    "над которым применяются, и не создают никаких других тензоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим тензор из единиц\n",
    "t = torch.ones((2, 3))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# И занулим его. Обратите внимание на нижнее подчеркивание.\n",
    "t.zero_()\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В torch все in-place операции строятся как обычные с нижним подчеркиванием (`_`) в конце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12648521.,  3275686.,    84453.],\n",
      "        [ 5147423.,  1954303., 15271690.]])\n",
      "tensor([[10804659., 11863032., 11041448.],\n",
      "        [ 8242561., 14953591.,  2428168.]])\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "t.random_()\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place операции позволяют сэкономить память, т.к. создаем в два раза меньше тензоров.\n",
    "У этого есть обратная сторона: если мы передаем тензор в функцию, то функция может\n",
    "этот тензор \"испортить\", поменяв его in-place.\n",
    "\n",
    "Посмотрим на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_tensor до функции:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Рандомный тензор того же размера:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 0., 1.]])\n",
      "zero_tensor после функции:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def random_like(a: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Создать случайный тензор того же размера, что и `a`.\"\"\"\n",
    "    # перезатираем `a` - это не очень хорошо\n",
    "    return a.random_(0, 5)\n",
    "\n",
    "\n",
    "zero_tensor = torch.zeros((2, 3))\n",
    "print(\"zero_tensor до функции:\")\n",
    "print(zero_tensor)\n",
    "\n",
    "random_tensor = random_like(zero_tensor)\n",
    "print(\"Рандомный тензор того же размера:\")\n",
    "print(random_tensor)\n",
    "\n",
    "print(\"zero_tensor после функции:\")\n",
    "print(zero_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_tensor до функции:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Рандомный тензор того же размера:\n",
      "tensor([[3., 0., 1.],\n",
      "        [1., 2., 4.]])\n",
      "zero_tensor после функции:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# без перезатирания\n",
    "def random_like(a: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.randint(0, 5, a.shape, dtype=torch.float32)\n",
    "    # еще можно одной строкой\n",
    "    # return torch.randint_like(a, 0, 5)\n",
    "    # у многих функций есть _like аналоги: zero_like, ones_like\n",
    "\n",
    "\n",
    "zero_tensor = torch.zeros((2, 3))\n",
    "print(\"zero_tensor до функции:\")\n",
    "print(zero_tensor)\n",
    "\n",
    "random_tensor = random_like(zero_tensor)\n",
    "print(\"Рандомный тензор того же размера:\")\n",
    "print(random_tensor)\n",
    "\n",
    "print(\"zero_tensor после функции:\")\n",
    "print(zero_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Операции с тензором\n",
    "Тензор создали, что же с ним можно делать дальше?\n",
    "Много чего. Мы рассмотрим несколько типов операций:\n",
    "1. Бинарные — как два тензора могут взаимодействовать.\n",
    "2. Индексирование — как нарезать тензор на куски.\n",
    "3. Продвинутое создание и индексирование — закрепим знания.\n",
    "\n",
    "Первая причина популярности PyTorch: numpy-подобный интерфейс, к которому быстро привыкаешь.\n",
    "Посмотрим, что нам предлагает этот инструмент.\n",
    "##### Бинарные операции\n",
    "Тензоры в PyTorch умеют делать те же операции, что и в NumPy: сложение, умножение, возведение в степень и т.д.\n",
    "Посмотрим на некоторые из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5.],\n",
       "        [5., 5., 5.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тензоры одинаковой размерности можно сложить\n",
    "2 * torch.ones((2, 3)) + 3 * torch.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[2., 3., 2.],\n",
      "        [3., 3., 3.],\n",
      "        [2., 3., 2.]])\n",
      "tensor([[2., 0., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Можно умножать поэлементно\n",
    "a = torch.eye(3)\n",
    "print(a)\n",
    "b = torch.randint_like(a, 2, 4)\n",
    "print(b)\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5]])\n",
      "torch.Size([1, 2])\n",
      "tensor([[2, 4],\n",
      "        [4, 2]])\n",
      "tensor([[26, 22]])\n",
      "tensor([[26, 22]])\n"
     ]
    }
   ],
   "source": [
    "# Можно умножить матричным умножением\n",
    "# Обратите внимание на torch.tensor с маленькой буквы — так можно создавать тензор из списка.\n",
    "# каждая вложенность списка — это новая размерность тензора.\n",
    "a = torch.tensor([[3, 5]])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [2, 4],\n",
    "        [4, 2],\n",
    "    ]\n",
    ")\n",
    "print(b)\n",
    "\"\"\"\n",
    "         2 | 4\n",
    "[3, 5] *   |    = [3*2 + 5*4, 3*4 + 5*2]\n",
    "         4 | 2\n",
    "\"\"\"\n",
    "# оператор @\n",
    "print(a @ b)\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39)\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Оператор @ также делает скалярное умножение векторов.\n",
    "# Результат тоже будет тензором - нуль-мерным тензором.\n",
    "# Чтобы превратить его в число, используется .item()\n",
    "p = torch.tensor([3, 4]) @ torch.tensor([5, 6])\n",
    "print(p)\n",
    "print(p.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Индексирования\n",
    "Индексирование в PyTorch работает так же, как в NumPy — те же квадратные скобки, те же `:`.\n",
    "Посмотрим на примеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n",
      "tensor([4, 5])\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(\n",
    "    [\n",
    "        [2, 3],\n",
    "        [4, 5],\n",
    "        [6, 7],\n",
    "    ]\n",
    ")\n",
    "print(a)\n",
    "# Взять строку с индексом 1 (нумерация идет с нуля)\n",
    "print(a[1])\n",
    "# Взять строку с индексом 1, а в ней - то, что по индексу 0\n",
    "print(a[1, 0])\n",
    "# Удобнее думать так:\n",
    "# - была размерность (3, 2), берем по индексу 1 вдоль первой оси\n",
    "# - остается размерность (2,), берем по индексу 0 вдоль первой оси\n",
    "# - остается одно число - это 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "tensor([[[[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 2.],\n",
      "          [1., 2.],\n",
      "          [1., 2.],\n",
      "          [1., 2.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 2.],\n",
      "          [1., 2.],\n",
      "          [1., 2.],\n",
      "          [1., 2.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 2.],\n",
      "          [1., 2.],\n",
      "          [1., 2.],\n",
      "          [1., 2.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Можно присваивать через те же [].\n",
    "# Чтобы сказать \"все значения\", используем :\n",
    "a = torch.ones((3, 5, 4, 2))\n",
    "a[:, 3, :, 1] = 2\n",
    "print(a[:, 3, :, 1])\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 2., 2., 0.],\n",
      "        [0., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Можно забирать отрезок из тензора.\n",
    "# Левый конец входит, правый не входит.\n",
    "a = torch.zeros((3, 5))\n",
    "print(a)\n",
    "a[0:2, 2:4] = 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Продвинутое создание и индексирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 6, 8])\n",
      "tensor([2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([10,  8,  6,  4,  2,  0, -2])\n"
     ]
    }
   ],
   "source": [
    "# Тензор с числами в диапазоне.\n",
    "# Левая граница входит, правая не входит.\n",
    "a = torch.arange(2, 9, 2)\n",
    "print(a)\n",
    "a = torch.arange(2, 9)\n",
    "print(a)\n",
    "a = torch.arange(10, -4, -2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В DL часто нужно состыковать размерности на стыке слоев нейросети.\n",
    "Для этого приходится добавлять размерность тензору, либо же его \"повернуть на бок\".\n",
    "\n",
    "Посмотрим, как это делается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 3,  2],\n",
       "        [ 6,  4],\n",
       "        [ 9,  6],\n",
       "        [12,  8]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "# None в индексировании добавляет ось\n",
    "# -1 в reshape говорит \"сам угадай, сколько по оси элементов\"\n",
    "a[:, None] @ torch.tensor([3, 2]).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[3., 2.],\n",
      "        [3., 2.],\n",
      "        [3., 2.],\n",
      "        [3., 2.],\n",
      "        [3., 2.]])\n",
      "tensor([[ 0.,  0.],\n",
      "        [ 3.,  2.],\n",
      "        [ 6.,  4.],\n",
      "        [ 9.,  6.],\n",
      "        [12.,  8.]])\n"
     ]
    }
   ],
   "source": [
    "# Попробую понять что делается\n",
    "a = torch.arange(5)\n",
    "print(a[:, None])\n",
    "b = torch.tensor([3, 2]) * torch.ones(5, 2)\n",
    "print(b)\n",
    "print(a[:, None] * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[3, 2]])\n",
      "tensor([[ 0,  0],\n",
      "        [ 3,  2],\n",
      "        [ 6,  4],\n",
      "        [ 9,  6],\n",
      "        [12,  8]])\n"
     ]
    }
   ],
   "source": [
    "# Но это не то же самое. Нунжо матричное умножение [5 x 1] matmul [1 x 2] -> [5 x 2]\n",
    "a = torch.arange(5)\n",
    "print(a[:, None])\n",
    "b = torch.tensor([3, 2])\n",
    "print(b[None, :])\n",
    "print(a[:, None] * b[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10,  15],\n",
      "        [ 38,  43],\n",
      "        [112, 117]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(5 * 5 * 5).reshape((5, 5, 5))\n",
    "# При индексировании можно явно указать, какие элементы из какого слоя хотим.\n",
    "# Учтите, что \"дырок\" в результате быть не должно\n",
    "print(a[[0, 1, 4], 2:4, [0, 3, 2]])\n",
    "# Не сработает, т.к. в последней оси взяли 2 элемента, а в первой 3\n",
    "# print(a[[0, 1, 4], 2:4, [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  8],\n",
      "        [ 9, 17],\n",
      "        [18, 26]])\n",
      "tensor([[[ 0,  2],\n",
      "         [ 6,  8]],\n",
      "\n",
      "        [[ 9, 11],\n",
      "         [15, 17]],\n",
      "\n",
      "        [[18, 20],\n",
      "         [24, 26]]])\n",
      "tensor([[[ 0,  2],\n",
      "         [ 6,  8]],\n",
      "\n",
      "        [[ 9, 11],\n",
      "         [15, 17]],\n",
      "\n",
      "        [[18, 20],\n",
      "         [24, 26]]])\n"
     ]
    }
   ],
   "source": [
    "# Возьму поменьше элементов\n",
    "a = torch.arange(3 * 3 * 3).reshape((3, 3, 3))\n",
    "print(a)\n",
    "# При индексировании можно явно указать, какие элементы из какого слоя хотим.\n",
    "# print(a[[0, 2], :2, [1, 2]])\n",
    "print(a[:, [0, 2], [0, 2]])\n",
    "print(a[:, [0, 2]][:, :, [0, 2]])\n",
    "layer_idx = [0, 1, 2]\n",
    "row_idx = [0, 2]\n",
    "col_idx = [0, 2]\n",
    "print(a[layer_idx][:, row_idx][:, :, col_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 9, 10, 11],\n",
      "         [18, 19, 20]],\n",
      "\n",
      "        [[ 3,  4,  5],\n",
      "         [12, 13, 14],\n",
      "         [21, 22, 23]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [15, 16, 17],\n",
      "         [24, 25, 26]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.testing import assert_close\n",
    "\n",
    "# Матрицу можно транспонировать\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "print(a.T)\n",
    "# А если это тензор, то при транспонировании лучше указать две оси\n",
    "a = torch.arange(27).reshape((3, 3, 3))\n",
    "print(a)\n",
    "print(a.transpose(0, 1))\n",
    "\n",
    "# a.transpose(0, 1) - это то же самое, что\n",
    "result = torch.zeros_like(a)\n",
    "for i in range(a.shape[2]):\n",
    "    result[:, :, i] = a[:, :, i].T\n",
    "# assert_close проверяет тензоры на равенство.\n",
    "assert_close(result, a.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m assert_close(actual: Any, expected: Any, *, allow_subclasses: bool = True, rtol: Optional[float] = None, atol: Optional[float] = None, equal_nan: bool = False, check_device: bool = True, check_dtype: bool = True, check_layout: bool = True, check_stride: bool = False, msg: Union[str, Callable[[str], str], NoneType] = None)\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Asserts that ``actual`` and ``expected`` are close.\n",
      "\n",
      "If ``actual`` and ``expected`` are strided, non-quantized, real-valued, and finite, they are considered close if\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\lvert \\text{actual} - \\text{expected} \\rvert \\le \\texttt{atol} + \\texttt{rtol} \\cdot \\lvert \\text{expected} \\rvert\n",
      "\n",
      "Non-finite values (``-inf`` and ``inf``) are only considered close if and only if they are equal. ``NaN``'s are\n",
      "only considered equal to each other if ``equal_nan`` is ``True``.\n",
      "\n",
      "In addition, they are only considered close if they have the same\n",
      "\n",
      "- :attr:`~torch.Tensor.device` (if ``check_device`` is ``True``),\n",
      "- ``dtype`` (if ``check_dtype`` is ``True``),\n",
      "- ``layout`` (if ``check_layout`` is ``True``), and\n",
      "- stride (if ``check_stride`` is ``True``).\n",
      "\n",
      "If either ``actual`` or ``expected`` is a meta tensor, only the attribute checks will be performed.\n",
      "\n",
      "If ``actual`` and ``expected`` are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are\n",
      "checked individually. Indices, namely ``indices`` for COO, ``crow_indices`` and ``col_indices`` for CSR and BSR,\n",
      "or ``ccol_indices``  and ``row_indices`` for CSC and BSC layouts, respectively,\n",
      "are always checked for equality whereas the values are checked for closeness according to the definition above.\n",
      "\n",
      "If ``actual`` and ``expected`` are quantized, they are considered close if they have the same\n",
      ":meth:`~torch.Tensor.qscheme` and the result of :meth:`~torch.Tensor.dequantize` is close according to the\n",
      "definition above.\n",
      "\n",
      "``actual`` and ``expected`` can be :class:`~torch.Tensor`'s or any tensor-or-scalar-likes from which\n",
      ":class:`torch.Tensor`'s can be constructed with :func:`torch.as_tensor`. Except for Python scalars the input types\n",
      "have to be directly related. In addition, ``actual`` and ``expected`` can be :class:`~collections.abc.Sequence`'s\n",
      "or :class:`~collections.abc.Mapping`'s in which case they are considered close if their structure matches and all\n",
      "their elements are considered close according to the above definition.\n",
      "\n",
      ".. note::\n",
      "\n",
      "    Python scalars are an exception to the type relation requirement, because their :func:`type`, i.e.\n",
      "    :class:`int`, :class:`float`, and :class:`complex`, is equivalent to the ``dtype`` of a tensor-like. Thus,\n",
      "    Python scalars of different types can be checked, but require ``check_dtype=False``.\n",
      "\n",
      "Args:\n",
      "    actual (Any): Actual input.\n",
      "    expected (Any): Expected input.\n",
      "    allow_subclasses (bool): If ``True`` (default) and except for Python scalars, inputs of directly related types\n",
      "        are allowed. Otherwise type equality is required.\n",
      "    rtol (Optional[float]): Relative tolerance. If specified ``atol`` must also be specified. If omitted, default\n",
      "        values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.\n",
      "    atol (Optional[float]): Absolute tolerance. If specified ``rtol`` must also be specified. If omitted, default\n",
      "        values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.\n",
      "    equal_nan (Union[bool, str]): If ``True``, two ``NaN`` values will be considered equal.\n",
      "    check_device (bool): If ``True`` (default), asserts that corresponding tensors are on the same\n",
      "        :attr:`~torch.Tensor.device`. If this check is disabled, tensors on different\n",
      "        :attr:`~torch.Tensor.device`'s are moved to the CPU before being compared.\n",
      "    check_dtype (bool): If ``True`` (default), asserts that corresponding tensors have the same ``dtype``. If this\n",
      "        check is disabled, tensors with different ``dtype``'s are promoted  to a common ``dtype`` (according to\n",
      "        :func:`torch.promote_types`) before being compared.\n",
      "    check_layout (bool): If ``True`` (default), asserts that corresponding tensors have the same ``layout``. If this\n",
      "        check is disabled, tensors with different ``layout``'s are converted to strided tensors before being\n",
      "        compared.\n",
      "    check_stride (bool): If ``True`` and corresponding tensors are strided, asserts that they have the same stride.\n",
      "    msg (Optional[Union[str, Callable[[str], str]]]): Optional error message to use in case a failure occurs during\n",
      "        the comparison. Can also passed as callable in which case it will be called with the generated message and\n",
      "        should return the new message.\n",
      "\n",
      "Raises:\n",
      "    ValueError: If no :class:`torch.Tensor` can be constructed from an input.\n",
      "    ValueError: If only ``rtol`` or ``atol`` is specified.\n",
      "    AssertionError: If corresponding inputs are not Python scalars and are not directly related.\n",
      "    AssertionError: If ``allow_subclasses`` is ``False``, but corresponding inputs are not Python scalars and have\n",
      "        different types.\n",
      "    AssertionError: If the inputs are :class:`~collections.abc.Sequence`'s, but their length does not match.\n",
      "    AssertionError: If the inputs are :class:`~collections.abc.Mapping`'s, but their set of keys do not match.\n",
      "    AssertionError: If corresponding tensors do not have the same :attr:`~torch.Tensor.shape`.\n",
      "    AssertionError: If ``check_layout`` is ``True``, but corresponding tensors do not have the same\n",
      "        :attr:`~torch.Tensor.layout`.\n",
      "    AssertionError: If only one of corresponding tensors is quantized.\n",
      "    AssertionError: If corresponding tensors are quantized, but have different :meth:`~torch.Tensor.qscheme`'s.\n",
      "    AssertionError: If ``check_device`` is ``True``, but corresponding tensors are not on the same\n",
      "        :attr:`~torch.Tensor.device`.\n",
      "    AssertionError: If ``check_dtype`` is ``True``, but corresponding tensors do not have the same ``dtype``.\n",
      "    AssertionError: If ``check_stride`` is ``True``, but corresponding strided tensors do not have the same stride.\n",
      "    AssertionError: If the values of corresponding tensors are not close according to the definition above.\n",
      "\n",
      "The following table displays the default ``rtol`` and ``atol`` for different ``dtype``'s. In case of mismatching\n",
      "``dtype``'s, the maximum of both tolerances is used.\n",
      "\n",
      "+---------------------------+------------+----------+\n",
      "| ``dtype``                 | ``rtol``   | ``atol`` |\n",
      "+===========================+============+==========+\n",
      "| :attr:`~torch.float16`    | ``1e-3``   | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.bfloat16`   | ``1.6e-2`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.float32`    | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.float64`    | ``1e-7``   | ``1e-7`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.complex32`  | ``1e-3``   | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.complex64`  | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.complex128` | ``1e-7``   | ``1e-7`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.quint8`     | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.quint2x4`   | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.quint4x2`   | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.qint8`      | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| :attr:`~torch.qint32`     | ``1.3e-6`` | ``1e-5`` |\n",
      "+---------------------------+------------+----------+\n",
      "| other                     | ``0.0``    | ``0.0``  |\n",
      "+---------------------------+------------+----------+\n",
      "\n",
      ".. note::\n",
      "\n",
      "    :func:`~torch.testing.assert_close` is highly configurable with strict default settings. Users are encouraged\n",
      "    to :func:`~functools.partial` it to fit their use case. For example, if an equality check is needed, one might\n",
      "    define an ``assert_equal`` that uses zero tolerances for every ``dtype`` by default:\n",
      "\n",
      "    >>> import functools\n",
      "    >>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n",
      "    >>> assert_equal(1e-9, 1e-10)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    AssertionError: Scalars are not equal!\n",
      "    <BLANKLINE>\n",
      "    Expected 1e-10 but got 1e-09.\n",
      "    Absolute difference: 9.000000000000001e-10\n",
      "    Relative difference: 9.0\n",
      "\n",
      "Examples:\n",
      "    >>> # tensor to tensor comparison\n",
      "    >>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n",
      "    >>> actual = torch.acos(torch.cos(expected))\n",
      "    >>> torch.testing.assert_close(actual, expected)\n",
      "\n",
      "    >>> # scalar to scalar comparison\n",
      "    >>> import math\n",
      "    >>> expected = math.sqrt(2.0)\n",
      "    >>> actual = 2.0 / math.sqrt(2.0)\n",
      "    >>> torch.testing.assert_close(actual, expected)\n",
      "\n",
      "    >>> # numpy array to numpy array comparison\n",
      "    >>> import numpy as np\n",
      "    >>> expected = np.array([1e0, 1e-1, 1e-2])\n",
      "    >>> actual = np.arccos(np.cos(expected))\n",
      "    >>> torch.testing.assert_close(actual, expected)\n",
      "\n",
      "    >>> # sequence to sequence comparison\n",
      "    >>> import numpy as np\n",
      "    >>> # The types of the sequences do not have to match. They only have to have the same\n",
      "    >>> # length and their elements have to match.\n",
      "    >>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]\n",
      "    >>> actual = tuple(expected)\n",
      "    >>> torch.testing.assert_close(actual, expected)\n",
      "\n",
      "    >>> # mapping to mapping comparison\n",
      "    >>> from collections import OrderedDict\n",
      "    >>> import numpy as np\n",
      "    >>> foo = torch.tensor(1.0)\n",
      "    >>> bar = 2.0\n",
      "    >>> baz = np.array(3.0)\n",
      "    >>> # The types and a possible ordering of mappings do not have to match. They only\n",
      "    >>> # have to have the same set of keys and their elements have to match.\n",
      "    >>> expected = OrderedDict([(\"foo\", foo), (\"bar\", bar), (\"baz\", baz)])\n",
      "    >>> actual = {\"baz\": baz, \"bar\": bar, \"foo\": foo}\n",
      "    >>> torch.testing.assert_close(actual, expected)\n",
      "\n",
      "    >>> expected = torch.tensor([1.0, 2.0, 3.0])\n",
      "    >>> actual = expected.clone()\n",
      "    >>> # By default, directly related instances can be compared\n",
      "    >>> torch.testing.assert_close(torch.nn.Parameter(actual), expected)\n",
      "    >>> # This check can be made more strict with allow_subclasses=False\n",
      "    >>> torch.testing.assert_close(\n",
      "    ...     torch.nn.Parameter(actual), expected, allow_subclasses=False\n",
      "    ... )\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: No comparison pair was able to handle inputs of type\n",
      "    <class 'torch.nn.parameter.Parameter'> and <class 'torch.Tensor'>.\n",
      "    >>> # If the inputs are not directly related, they are never considered close\n",
      "    >>> torch.testing.assert_close(actual.numpy(), expected)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>\n",
      "    and <class 'torch.Tensor'>.\n",
      "    >>> # Exceptions to these rules are Python scalars. They can be checked regardless of\n",
      "    >>> # their type if check_dtype=False.\n",
      "    >>> torch.testing.assert_close(1.0, 1, check_dtype=False)\n",
      "\n",
      "    >>> # NaN != NaN by default.\n",
      "    >>> expected = torch.tensor(float(\"Nan\"))\n",
      "    >>> actual = expected.clone()\n",
      "    >>> torch.testing.assert_close(actual, expected)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    AssertionError: Scalars are not close!\n",
      "    <BLANKLINE>\n",
      "    Expected nan but got nan.\n",
      "    Absolute difference: nan (up to 1e-05 allowed)\n",
      "    Relative difference: nan (up to 1.3e-06 allowed)\n",
      "    >>> torch.testing.assert_close(actual, expected, equal_nan=True)\n",
      "\n",
      "    >>> expected = torch.tensor([1.0, 2.0, 3.0])\n",
      "    >>> actual = torch.tensor([1.0, 4.0, 5.0])\n",
      "    >>> # The default error message can be overwritten.\n",
      "    >>> torch.testing.assert_close(\n",
      "    ...     actual, expected, msg=\"Argh, the tensors are not close!\"\n",
      "    ... )\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    AssertionError: Argh, the tensors are not close!\n",
      "    >>> # If msg is a callable, it can be used to augment the generated message with\n",
      "    >>> # extra information\n",
      "    >>> torch.testing.assert_close(\n",
      "    ...     actual, expected, msg=lambda msg: f\"Header\\n\\n{msg}\\n\\nFooter\"\n",
      "    ... )\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    AssertionError: Header\n",
      "    <BLANKLINE>\n",
      "    Tensor-likes are not close!\n",
      "    <BLANKLINE>\n",
      "    Mismatched elements: 2 / 3 (66.7%)\n",
      "    Greatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)\n",
      "    Greatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)\n",
      "    <BLANKLINE>\n",
      "    Footer\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.12/dist-packages/torch/testing/_comparison.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "assert_close?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "transpose(input, dim0, dim1) -> Tensor\n",
      "\n",
      "Returns a tensor that is a transposed version of :attr:`input`.\n",
      "The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.\n",
      "\n",
      "If :attr:`input` is a strided tensor then the resulting :attr:`out`\n",
      "tensor shares its underlying storage with the :attr:`input` tensor, so\n",
      "changing the content of one would change the content of the other.\n",
      "\n",
      "If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` then the\n",
      "resulting :attr:`out` tensor *does not* share the underlying storage\n",
      "with the :attr:`input` tensor.\n",
      "\n",
      "If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` with compressed\n",
      "layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments\n",
      ":attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must\n",
      "both be sparse dimensions. The batch dimensions of a sparse tensor are the\n",
      "dimensions preceding the sparse dimensions.\n",
      "\n",
      ".. note::\n",
      "    Transpositions which interchange the sparse dimensions of a `SparseCSR`\n",
      "    or `SparseCSC` layout tensor will result in the layout changing between\n",
      "    the two options. Transposition of the sparse dimensions of a ` SparseBSR`\n",
      "    or `SparseBSC` layout tensor will likewise generate a result with the\n",
      "    opposite layout.\n",
      "\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "    dim0 (int): the first dimension to be transposed\n",
      "    dim1 (int): the second dimension to be transposed\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(2, 3)\n",
      "    >>> x\n",
      "    tensor([[ 1.0028, -0.9893,  0.5809],\n",
      "            [-0.1669,  0.7299,  0.4942]])\n",
      "    >>> torch.transpose(x, 0, 1)\n",
      "    tensor([[ 1.0028, -0.1669],\n",
      "            [-0.9893,  0.7299],\n",
      "            [ 0.5809,  0.4942]])\n",
      "\n",
      "See also :func:`torch.t`.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "torch.transpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функции над тензором\n",
    "Большинство функций, которые есть в NumPy над матрицами,\n",
    "есть и в PyTorch над тензорами.\n",
    "Рассмотрим самые популярные из этих функций:\n",
    "- сложение, умножение, вычитание, деление;\n",
    "- матричное умножение;\n",
    "- обращение тензора;\n",
    "\n",
    "Также разберем нетривиальные моменты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:08:47.146592Z",
     "iopub.status.busy": "2026-02-04T04:08:47.146227Z",
     "iopub.status.idle": "2026-02-04T04:08:47.271328Z",
     "shell.execute_reply": "2026-02-04T04:08:47.270182Z",
     "shell.execute_reply.started": "2026-02-04T04:08:47.146563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поэлементное умножение:\n",
      "tensor([[ 0,  5],\n",
      "        [12, 21]])\n",
      "То же самое, что '*':\n",
      "tensor([[ 0,  5],\n",
      "        [12, 21]])\n",
      "То же самое, что \"+\", есть inplace-версия a.add_(b):\n",
      "tensor([[ 4,  6],\n",
      "        [ 8, 10]])\n",
      "Возвести в квадрат поэлементно:\n",
      "tensor([[0, 1],\n",
      "        [4, 9]])\n",
      "Обратите внимание: в математической литературе A^2 - это не поэлементно\n",
      "В матем. литературе обычно под A^2 подразумевают следующее:\n",
      "tensor([[ 2,  3],\n",
      "        [ 6, 11]])\n",
      "В частности, обратную матрицу надо считать вот так:\n",
      "tensor([[-1.5000,  0.5000],\n",
      "        [ 1.0000,  0.0000]])\n",
      "Но вот a**(-1) лишь каждый элемент обратит:\n",
      "tensor([[   inf, 1.0000],\n",
      "        [0.5000, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Тензоры можно возводить в степень, умножать друг на друга.\n",
    "# Это будет поэлементно!\n",
    "# Вообще, большинство арифметических операций в pytorch выполняются поэлементно.\n",
    "# Это +, -, *, /\n",
    "a = torch.arange(4).reshape((2, 2))\n",
    "b = torch.arange(4, 8).reshape((2, 2))\n",
    "print(\"Поэлементное умножение:\")\n",
    "print(a * b)\n",
    "print(\"То же самое, что '*':\")\n",
    "print(a.mul(b))\n",
    "print('То же самое, что \"+\", есть inplace-версия a.add_(b):')\n",
    "print(a.add(b))\n",
    "print(\"Возвести в квадрат поэлементно:\")\n",
    "print(a**2)\n",
    "print(\"Обратите внимание: в математической литературе A^2 - это не поэлементно\")\n",
    "print(\"В матем. литературе обычно под A^2 подразумевают следующее:\")\n",
    "print(a @ a)\n",
    "print(\"В частности, обратную матрицу надо считать вот так:\")\n",
    "print(a.float().inverse())  # приводим к float, т.к. inverse работает только с ним\n",
    "print(\"Но вот a**(-1) лишь каждый элемент обратит:\")\n",
    "print(a.float() ** (-1))  # pytorch не дает обращать integer элементы тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиенты в PyTorch\n",
    "Вторая причина популярности PyTorch: удобная работа с производными операций.\n",
    "\n",
    "PyTorch умеет считать градиенты автоматически.\n",
    "Вы делаете любое вычисление, например:\n",
    "```python\n",
    "result = my_matrix ** 2\n",
    "# затем\n",
    "result.backward()\n",
    "```\n",
    "Более подробная [документация](https://pytorch.org/docs/stable/notes/autograd.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:13:30.925657Z",
     "iopub.status.busy": "2026-02-04T04:13:30.924693Z",
     "iopub.status.idle": "2026-02-04T04:13:30.934997Z",
     "shell.execute_reply": "2026-02-04T04:13:30.933970Z",
     "shell.execute_reply.started": "2026-02-04T04:13:30.925620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [2., 2.]], dtype=torch.float64, requires_grad=True)\n",
      "None\n",
      "tensor([[ 8.],\n",
      "        [16.]], dtype=torch.float64, grad_fn=<MmBackward0>)\n",
      "tensor([[5., 3.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# requires_grad=True означает, что мы хотим считать градиент по всем элементам тензора\n",
    "w = torch.tensor([[1, 1], [2, 2]], dtype=float, requires_grad=True)\n",
    "x = torch.tensor([[5], [3]], dtype=float)\n",
    "print(w)\n",
    "print(w.grad)\n",
    "final_answer = w @ x\n",
    "print(final_answer)\n",
    "one_scalar = final_answer.sum()\n",
    "one_scalar.backward()\n",
    "print(w.grad)\n",
    "# градиент можно брать только от скаляров, следующая строка не сработает:\n",
    "# final_answer.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь вручную считаем:\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        w_{11} & w_{12} \\\\\n",
    "        w_{21} & w_{22}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "        w_{11} x_1 + w_{12} x_2 \\\\\n",
    "        w_{21} x_1 + w_{22} x_2\n",
    "    \\end{bmatrix}\n",
    "    \\xrightarrow{\\sum}\n",
    "    w_{11} x_1 + w_{12} x_2 + w_{21} x_1 + w_{22} x_2\n",
    "$$\n",
    "отсюда видим, что\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{11}} = x_1; \\quad \n",
    "\\frac{\\partial L}{\\partial w_{12}} = x_2; \\quad \n",
    "\\frac{\\partial L}{\\partial w_{21}} = x_1; \\quad \n",
    "\\frac{\\partial L}{\\partial w_{22}} = x_2; \\quad \n",
    "$$\n",
    "Смотрим на числа выше и убеждаемся, что градиент был подсчитан верно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:15:59.601450Z",
     "iopub.status.busy": "2026-02-04T04:15:59.600535Z",
     "iopub.status.idle": "2026-02-04T04:15:59.618147Z",
     "shell.execute_reply": "2026-02-04T04:15:59.616937Z",
     "shell.execute_reply.started": "2026-02-04T04:15:59.601405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "# Градиент не будет работать без requires_grad=True\n",
    "a = torch.tensor([1.0, 2.0])\n",
    "try:\n",
    "    torch.sum(a).backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:17:40.092291Z",
     "iopub.status.busy": "2026-02-04T04:17:40.091932Z",
     "iopub.status.idle": "2026-02-04T04:17:40.100982Z",
     "shell.execute_reply": "2026-02-04T04:17:40.100024Z",
     "shell.execute_reply.started": "2026-02-04T04:17:40.092262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "tensor([2.4000, 2.4000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/607681094.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(b.grad)\n"
     ]
    }
   ],
   "source": [
    "# Чтобы суметь подсчитать градиент,\n",
    "# pytorch сохраняет все промежуточные результаты.\n",
    "# Иногда не нужно считать градиент (даже если requires_grad=True)\n",
    "# В таком случае можно попросить не хранить эти промежуточные результаты.\n",
    "# Это экономит память.\n",
    "a = torch.tensor([1.0, 0.2], requires_grad=True)\n",
    "b = a.sum()\n",
    "c = b ** 2\n",
    "c.backward()\n",
    "print(b.grad)\n",
    "print()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:27:40.408134Z",
     "iopub.status.busy": "2026-02-04T04:27:40.407785Z",
     "iopub.status.idle": "2026-02-04T04:27:40.418571Z",
     "shell.execute_reply": "2026-02-04T04:27:40.417552Z",
     "shell.execute_reply.started": "2026-02-04T04:27:40.408103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4000)\n",
      "\n",
      "tensor([2.4000, 2.4000])\n"
     ]
    }
   ],
   "source": [
    "# Чтобы суметь подсчитать градиент,\n",
    "# pytorch сохраняет все промежуточные результаты.\n",
    "# Иногда не нужно считать градиент (даже если requires_grad=True)\n",
    "# В таком случае можно попросить не хранить эти промежуточные результаты.\n",
    "# Это экономит память.\n",
    "a = torch.tensor([1.0, 0.2], requires_grad=True)\n",
    "b = a.sum()\n",
    "b.retain_grad()  # for non-leaf tensors!\n",
    "c = b ** 2\n",
    "c.backward()\n",
    "print(b.grad)\n",
    "print()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:22:15.972651Z",
     "iopub.status.busy": "2026-02-04T04:22:15.972282Z",
     "iopub.status.idle": "2026-02-04T04:22:15.980112Z",
     "shell.execute_reply": "2026-02-04T04:22:15.979257Z",
     "shell.execute_reply.started": "2026-02-04T04:22:15.972623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    b = a.sum()\n",
    "# уже не сработает - градиента не было\n",
    "try:\n",
    "    b.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([ 6., 16.])\n",
      "tensor([11., 24.])\n"
     ]
    }
   ],
   "source": [
    "# Если тензор участвует в нескольких вычислениях, то вызов .backwards() сложит градиенты\n",
    "w = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "x = torch.tensor([3.0, 4.0])\n",
    "# Производная loss_1 даст 2w*x = [6, 16]\n",
    "loss_1 = torch.sum(w**2 * x)\n",
    "# Производная loss_2 даст 2w + x = [2 + 3, 4 + 4] = [5, 8]\n",
    "loss_2 = torch.sum(w**2 + w * x + 2)\n",
    "print(w.grad)\n",
    "loss_1.backward()\n",
    "print(w.grad)\n",
    "loss_2.backward()\n",
    "# Две производные сложились: [6, 16] + [5, 8] = [11, 24]\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции потерь\n",
    "В обычном ML было много функций потерь: MSE, MAE, MAPE и так далее.\n",
    "Они есть и pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T04:43:42.565561Z",
     "iopub.status.busy": "2026-02-04T04:43:42.564697Z",
     "iopub.status.idle": "2026-02-04T04:43:42.578337Z",
     "shell.execute_reply": "2026-02-04T04:43:42.577148Z",
     "shell.execute_reply.started": "2026-02-04T04:43:42.565521Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.)\n",
      "tensor(3.5000)\n",
      "tensor(7.)\n",
      "Градиент (y_true - w @ x)^2:\n",
      "tensor([-4., -4.])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# (5 - 2)^2 = 9\n",
    "loss = F.mse_loss(torch.tensor([2.0]), torch.tensor([5.0]))\n",
    "print(loss)\n",
    "# mean(|2 - 5| + |4 - 0|) = mean(3, 4) = 3.5\n",
    "loss = F.l1_loss(torch.tensor([2.0, 4.0]), torch.tensor([5.0, 0.0]))\n",
    "print(loss)\n",
    "# можно не усреднять, а суммировать\n",
    "loss = F.l1_loss(torch.tensor([2.0, 4.0]), torch.tensor([5.0, 0.0]), reduction=\"sum\")\n",
    "print(loss)\n",
    "\n",
    "# От них можно так же брать градиент!\n",
    "w = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "y_true = torch.tensor(5.0)\n",
    "y_pred = w @ x\n",
    "loss = F.mse_loss(y_pred, y_true)\n",
    "loss.backward()\n",
    "print(\"Градиент (y_true - w @ x)^2:\")\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Слои нейросети\n",
    "На лекции мы узнали, что нейросети строятся из слоев.\n",
    "Есть ли слои в pytorch?\n",
    "\n",
    "Да, они есть — их много готовых.\n",
    "Это третья причина популярности PyTorch: многие слои из мира Deep Learning уже реализованы и готовы к использованию.\n",
    "\n",
    "В этом ноутбуке мы рассмотрим полносвязный слой, с остальными будем знакомиться в следующих уроках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:31.028734Z",
     "iopub.status.busy": "2026-02-04T05:19:31.028087Z",
     "iopub.status.idle": "2026-02-04T05:19:31.128384Z",
     "shell.execute_reply": "2026-02-04T05:19:31.127673Z",
     "shell.execute_reply.started": "2026-02-04T05:19:31.028708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9619],\n",
      "        [0.9619],\n",
      "        [0.9619],\n",
      "        [0.9619],\n",
      "        [0.9619],\n",
      "        [0.9619]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Слой вида y = w @ x + b\n",
    "# Веса инициализируются случайными числами\n",
    "# bias=False означает \"b=0 всегда\"\n",
    "lin_1 = nn.Linear(2, 1)\n",
    "# Принимает тензор размерности (bs, in_features)\n",
    "# bs - batch_size, размер батча\n",
    "# in_features - размерность каждого вектора, в нашем случае 2\n",
    "y = lin_1(torch.ones((6, 2)))\n",
    "print(y)\n",
    "y.retain_grad()\n",
    "f = y.sum()\n",
    "f.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:31.395056Z",
     "iopub.status.busy": "2026-02-04T05:19:31.394773Z",
     "iopub.status.idle": "2026-02-04T05:19:31.435596Z",
     "shell.execute_reply": "2026-02-04T05:19:31.434877Z",
     "shell.execute_reply.started": "2026-02-04T05:19:31.395033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
       "\n",
       "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
       "\n",
       "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
       "\n",
       "Args:\n",
       "    in_features: size of each input sample\n",
       "    out_features: size of each output sample\n",
       "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
       "        Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*, H_\\text{in})` where :math:`*` means any number of\n",
       "      dimensions including none and :math:`H_\\text{in} = \\text{in\\_features}`.\n",
       "    - Output: :math:`(*, H_\\text{out})` where all but the last dimension\n",
       "      are the same shape as the input and :math:`H_\\text{out} = \\text{out\\_features}`.\n",
       "\n",
       "Attributes:\n",
       "    weight: the learnable weights of the module of shape\n",
       "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
       "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
       "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
       "            If :attr:`bias` is ``True``, the values are initialized from\n",
       "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Linear(20, 30)\n",
       "    >>> input = torch.randn(128, 20)\n",
       "    >>> output = m(input)\n",
       "    >>> print(output.size())\n",
       "    torch.Size([128, 30])\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:34.218432Z",
     "iopub.status.busy": "2026-02-04T05:19:34.217867Z",
     "iopub.status.idle": "2026-02-04T05:19:34.224459Z",
     "shell.execute_reply": "2026-02-04T05:19:34.223798Z",
     "shell.execute_reply.started": "2026-02-04T05:19:34.218405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.5406, 0.5869]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.1657], requires_grad=True)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# У линейного слоя есть веса (weight) и смещение (bias), его можно получить\n",
    "print(lin_1.weight)\n",
    "print()\n",
    "print(lin_1.bias)\n",
    "print()\n",
    "lin_2 = nn.Linear(2, 1, bias=False)\n",
    "# bias будет None, если его отключить (см. выше)\n",
    "print(lin_2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:38.786247Z",
     "iopub.status.busy": "2026-02-04T05:19:38.785523Z",
     "iopub.status.idle": "2026-02-04T05:19:38.802291Z",
     "shell.execute_reply": "2026-02-04T05:19:38.801384Z",
     "shell.execute_reply.started": "2026-02-04T05:19:38.786217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311]])\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (1): Sigmoid()\n",
      ")\n",
      "\n",
      "tensor([[0.5532],\n",
      "        [0.5532],\n",
      "        [0.5532]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Но в лекции говорили, что нужна еще нелинейность.\n",
    "# Ее тоже можно найти в torch.nn\n",
    "act = nn.Sigmoid()\n",
    "print(act(torch.ones((2, 4))))\n",
    "print()\n",
    "# соберем все воедино\n",
    "# nn.Sequential позволяет задать несколько слоев подряд\n",
    "fc = nn.Sequential(nn.Linear(2, 1, bias=True), nn.Sigmoid())\n",
    "print(fc)\n",
    "print()\n",
    "print(fc(torch.ones((3, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:39.170909Z",
     "iopub.status.busy": "2026-02-04T05:19:39.170617Z",
     "iopub.status.idle": "2026-02-04T05:19:39.176689Z",
     "shell.execute_reply": "2026-02-04T05:19:39.176004Z",
     "shell.execute_reply.started": "2026-02-04T05:19:39.170878Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (i_am_layer): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (i_am_activation): Sigmoid()\n",
      ")\n",
      "\n",
      "Linear(in_features=2, out_features=1, bias=True)\n",
      "\n",
      "Linear(in_features=2, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# В Sequential можно задать имена слоям через OrderedDict\n",
    "from collections import OrderedDict\n",
    "\n",
    "fc = nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"i_am_layer\", nn.Linear(2, 1)),\n",
    "            (\"i_am_activation\", nn.Sigmoid()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(fc)\n",
    "print()\n",
    "# слои можно достать по имени (как поле) или по индексу\n",
    "print(fc[0])\n",
    "print()\n",
    "# ради такой возможности и заводят слои через OrderedDict\n",
    "print(fc.i_am_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch и видеокарта\n",
    "Четвертая причина популярности PyTorch: удобная работа с видеокартой.\n",
    "Давайте посмотрим, как это делается.\n",
    "\n",
    "С чем мы познакомимся:\n",
    "- операции `.to()`, `.cuda()`, `.cpu()` для ускорения вычислений\n",
    "- утилита `nvidia-smi` для отслеживания здоровья видеокарты\n",
    "- демонстрация скорости — насколько же видеокарта быстрее процессора?\n",
    "- какие бывают проблемы с видеокартой и как их решать на примере `device assert triggered`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:09:17.640528Z",
     "iopub.status.busy": "2026-02-04T05:09:17.640227Z",
     "iopub.status.idle": "2026-02-04T05:09:18.355764Z",
     "shell.execute_reply": "2026-02-04T05:09:18.355030Z",
     "shell.execute_reply.started": "2026-02-04T05:09:17.640502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3.])\n",
      "cpu\n",
      "\n",
      "tensor([1., 3.], device='cuda:0')\n",
      "cuda:0\n",
      "\n",
      "tensor([1., 3.])\n",
      "cpu\n",
      "\n",
      "tensor([1., 3.], device='cuda:0')\n",
      "cuda:0\n",
      "\n",
      "tensor([1., 3.])\n",
      "cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 3.0])\n",
    "print(a)\n",
    "# Каждый тензор лежит либо в RAM (она принадлежит CPU), либо в GPU - это можно узнать по .device\n",
    "print(a.device)\n",
    "print()\n",
    "# Тензор легко можно перенести на GPU\n",
    "a = a.to(\"cuda\")\n",
    "# Теперь у нас тензор на GPU.\n",
    "print(a)\n",
    "print(a.device)\n",
    "print()\n",
    "# А теперь - обратно на CPU\n",
    "a = a.to(\"cpu\")\n",
    "print(a)\n",
    "print(a.device)\n",
    "print()\n",
    "# Перенести на GPU можно также командой .cuda()\n",
    "# Если тензор уже на видеокарте, то .cuda() ничего не будет делать\n",
    "a = a.cuda()\n",
    "print(a)\n",
    "print(a.device)\n",
    "print()\n",
    "# Аналогично можно перенести на CPU через .cpu()\n",
    "a = a.cpu()\n",
    "print(a)\n",
    "print(a.device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:14:10.254838Z",
     "iopub.status.busy": "2026-02-04T05:14:10.254336Z",
     "iopub.status.idle": "2026-02-04T05:14:10.261445Z",
     "shell.execute_reply": "2026-02-04T05:14:10.260560Z",
     "shell.execute_reply.started": "2026-02-04T05:14:10.254810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2452876292.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Можно какие-то промежуточные значения гонять туда-сюда, но это будет медленно.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Так, код ниже не сработает.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Главное правило - нельзя перемешивать тензоры на видеокарте и тензоры на CPU.\n",
    "# Либо все операции на CPU, либо на GPU.\n",
    "# Можно какие-то промежуточные значения гонять туда-сюда, но это будет медленно.\n",
    "# Так, код ниже не сработает.\n",
    "torch.zeros((2, 3), device=\"cpu\") + torch.zeros((2, 3), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:14:21.764328Z",
     "iopub.status.busy": "2026-02-04T05:14:21.763697Z",
     "iopub.status.idle": "2026-02-04T05:14:21.770406Z",
     "shell.execute_reply": "2026-02-04T05:14:21.769678Z",
     "shell.execute_reply.started": "2026-02-04T05:14:21.764302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Главное правило - нельзя перемешивать тензоры на видеокарте и тензоры на CPU.\n",
    "# Либо все операции на CPU, либо на GPU.\n",
    "# Можно какие-то промежуточные значения гонять туда-сюда, но это будет медленно.\n",
    "# Так, код ниже не сработает.\n",
    "torch.zeros((2, 3), device=\"cuda:0\") + torch.zeros((2, 3), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:15:01.589406Z",
     "iopub.status.busy": "2026-02-04T05:15:01.588553Z",
     "iopub.status.idle": "2026-02-04T05:15:01.771672Z",
     "shell.execute_reply": "2026-02-04T05:15:01.770790Z",
     "shell.execute_reply.started": "2026-02-04T05:15:01.589375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  4 05:15:01 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             33W /  250W |     291MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU не безлимитна. Хочется узнать, сколько ресурсов GPU мы занимаем.\n",
    "# Для этого есть nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видим, что процесс python3.10 что-то занимает. Это как раз наши тензоры.\n",
    "# У вас будут другие числа - все сильно зависит от модели видеокарты и операционной системы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чем же такой плюс от работы на GPU?\n",
    "\n",
    "Давайте увидим сами. Возьмем большой тензор и начнем его умножать большое число раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:27:22.183946Z",
     "iopub.status.busy": "2026-02-04T05:27:22.183292Z",
     "iopub.status.idle": "2026-02-04T05:27:44.214712Z",
     "shell.execute_reply": "2026-02-04T05:27:44.214081Z",
     "shell.execute_reply.started": "2026-02-04T05:27:22.183917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7c4dc17bbfe0>\n",
      "many_layers(data)\n",
      "  3.04 s\n",
      "  1 measurement, 5 runs , 8 threads\n"
     ]
    }
   ],
   "source": [
    "# Используем benchmark из pytorch - он сделает \"честное\" вычисление (без кешей, с прогревом и т.п.)\n",
    "# Подробнее про benchmark в PyTorch: https://pytorch.org/tutorials/recipes/recipes/benchmark.html\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "num_channels = 1024\n",
    "many_layers = nn.Sequential(*[nn.Linear(num_channels, num_channels)] * 100)\n",
    "data = torch.randn((3000, num_channels))\n",
    "# Прогоним через слой 5 раз и подсчитаем среднее/дисперсию\n",
    "t = benchmark.Timer(\n",
    "    stmt=\"many_layers(data)\",\n",
    "    globals={\"many_layers\": many_layers, \"data\": data},\n",
    "    # заметьте, используем 8 ядер процессора\n",
    "    num_threads=8,\n",
    ")\n",
    "print(t.timeit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:29:00.692582Z",
     "iopub.status.busy": "2026-02-04T05:29:00.691928Z",
     "iopub.status.idle": "2026-02-04T05:29:01.273892Z",
     "shell.execute_reply": "2026-02-04T05:29:01.273293Z",
     "shell.execute_reply.started": "2026-02-04T05:29:00.692553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7c4dfffe52b0>\n",
      "many_layers_gpu(data_gpu)\n",
      "  80.02 ms\n",
      "  1 measurement, 5 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "# Повторим эксперимент, но уже на GPU\n",
    "\n",
    "many_layers_gpu = many_layers.to(\"cuda\")\n",
    "data_gpu = data.to(\"cuda\")\n",
    "t = benchmark.Timer(\n",
    "    stmt=\"many_layers_gpu(data_gpu)\",\n",
    "    globals={\"many_layers_gpu\": many_layers_gpu, \"data_gpu\": data_gpu},\n",
    ")\n",
    "print(t.timeit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 30 раз быстрее! И это только игрушечный пример.\n",
    "В реальных сетях разница может быть еще больше.\n",
    "\n",
    "В чем же подвох? Почему бы все не учить на видеокарте?\n",
    "Есть две причины, которые остановят нас:\n",
    "1. У видеокарты очень мало оперативной памяти по сравнению с сервером.\n",
    "На сервере вы можете поставить и 512 Гб оперативной памяти, и даже 1 Тб.\n",
    "Но на видеокарте сейчас можно ставить до 80 Гб (Tesla H800).\n",
    "2. На видеокарте не очень информативные ошибки. Это особенность программы CUDA,\n",
    "которую PyTorch использует для вычислений на видеокарте.\n",
    "\n",
    "Первая проблема фундаментальная, ее решают через распределение нагрузки по нескольким видеокартам.\n",
    "Это делать сложно: нужно научить несколько GPU взаимодействовать друг с другом, распределять равномерно нагрузку\n",
    "и синхронизировать работу.\n",
    "На первых порах вам вряд ли придется столкнуться с такой задачей, поэтому пока оставим эту тему.\n",
    "\n",
    "Рассмотрим вторую проблему с неинформативными ошибками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Подсчитаем Binary Cross Entropy loss с элементами вне {0, 1}\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/start-dl-fEQaQ9Q8-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/start-dl-fEQaQ9Q8-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/start-dl-fEQaQ9Q8-py3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/start-dl-fEQaQ9Q8-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# Подсчитаем Binary Cross Entropy loss с элементами вне {0, 1}\n",
    "nn.BCELoss()(torch.arange(2, 3).float(), torch.arange(2, 3).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:29:37.022789Z",
     "iopub.status.busy": "2026-02-04T05:29:37.022472Z",
     "iopub.status.idle": "2026-02-04T05:29:37.183001Z",
     "shell.execute_reply": "2026-02-04T05:29:37.182018Z",
     "shell.execute_reply.started": "2026-02-04T05:29:37.022764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:90: operator(): block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2270901221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# То же самое, но тензор на GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         return F.binary_cross_entropy(\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3528\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3530\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# То же самое, но тензор на GPU\n",
    "nn.BCELoss()(torch.arange(2, 3).float().cuda(), torch.arange(2, 3).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:30:53.945039Z",
     "iopub.status.busy": "2026-02-04T05:30:53.944680Z",
     "iopub.status.idle": "2026-02-04T05:30:53.975324Z",
     "shell.execute_reply": "2026-02-04T05:30:53.974511Z",
     "shell.execute_reply.started": "2026-02-04T05:30:53.945011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1648402148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Более того - после ошибки уже ничего нельзя сделать с видеокартой.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Придется перезапускать ноутбук :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Более того - после ошибки уже ничего нельзя сделать с видеокартой.\n",
    "# Придется перезапускать ноутбук :(\n",
    "torch.randn((2, 3), device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за таких проблем на видеокарте не рекомендуется отлаживать модели (об этом говорили в лекции)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.9.0+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
      "          [ 0.6784, -1.2345, -0.0431, -1.6047]],\n",
      "\n",
      "         [[-0.7521,  1.6487, -0.3925, -1.4036],\n",
      "          [-0.7279, -0.5594, -0.7688,  0.7624]],\n",
      "\n",
      "         [[ 1.6423, -0.1596, -0.4974,  0.4396],\n",
      "          [-0.7581,  1.0783,  0.8008,  1.6806]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2791,  1.2964,  0.6105,  1.3347],\n",
      "          [-0.2316,  0.0418, -0.2516,  0.8599]],\n",
      "\n",
      "         [[-1.3847, -0.8712, -0.2234,  1.7174],\n",
      "          [ 0.3189, -0.4245,  0.3057, -0.7746]],\n",
      "\n",
      "         [[-1.5576,  0.9956, -0.8798, -0.6011],\n",
      "          [-1.2742,  2.1228, -1.2347, -0.4879]]]])\n"
     ]
    }
   ],
   "source": [
    "# Создание тензора с произвольными данными\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "\n",
    "# Тензор заполненный нулями\n",
    "zeros = torch.zeros((5, 3))\n",
    "\n",
    "# Тензор заполненный единицами\n",
    "ones = torch.ones((2, 3, 4))\n",
    "\n",
    "# Тензор из нормального распределения\n",
    "randn = torch.randn((2, 3, 2, 4))\n",
    "\n",
    "print(randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "\n",
      "\n",
      "Returns a tensor filled with random numbers from a normal distribution\n",
      "with mean `0` and variance `1` (also called the standard normal\n",
      "distribution).\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
      "\n",
      "For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and\n",
      "unit variance as\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} \\sim \\mathcal{CN}(0, 1)\n",
      "\n",
      "This is equivalent to separately sampling the real :math:`(\\operatorname{Re})` and imaginary\n",
      ":math:`(\\operatorname{Im})` part of :math:`\\text{out}_i` as\n",
      "\n",
      ".. math::\n",
      "    \\operatorname{Re}(\\text{out}_{i}) \\sim \\mathcal{N}(0, \\frac{1}{2}),\\quad\n",
      "    \\operatorname{Im}(\\text{out}_{i}) \\sim \\mathcal{N}(0, \\frac{1}{2})\n",
      "\n",
      "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "\n",
      "\n",
      "Args:\n",
      "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "        Can be a variable number of arguments or a collection like a list or tuple.\n",
      "\n",
      "Keyword args:\n",
      "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "    out (Tensor, optional): the output tensor.\n",
      "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n",
      "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "        Default: ``torch.strided``.\n",
      "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "        Default: if ``None``, uses the current device for the default tensor type\n",
      "        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n",
      "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "    requires_grad (bool, optional): If autograd should record operations on the\n",
      "        returned tensor. Default: ``False``.\n",
      "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.randn(4)\n",
      "    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
      "    >>> torch.randn(2, 3)\n",
      "    tensor([[ 1.5954,  2.8929, -1.0923],\n",
      "            [ 1.1719, -0.4709, -0.1996]])\n",
      "\n",
      ".. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "torch.randn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.eye(3, requires_grad=True)\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([[1, 1], [2, 2]], dtype=float, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([[2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.]], dtype=torch.float64) tensor([[2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64) False\n",
      "element 0 of tensors does not require grad and does not have a grad_fn\n",
      "tensor([[2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "tensor([[2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1, 1], [2, 2]], dtype=float, requires_grad=True)\n",
    "loss_1 = (w**2).sum()\n",
    "print(loss_1)\n",
    "loss_1.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# Отклюючим градиенты\n",
    "# w = w.detach()  # Returns a new Tensor, detached from the current graph.\n",
    "# print(w, w.grad)\n",
    "\n",
    "w.requires_grad_(False)  # отключает градиент inplace\n",
    "print(w, w.grad, w.requires_grad)\n",
    "loss_2 = (w**4).sum()\n",
    "try:\n",
    "    loss_2.backward()  # неуспех\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "print(w.grad)\n",
    "\n",
    "w = w.requires_grad_(True)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Returns a new Tensor, detached from the current graph.\n",
      "\n",
      "The result will never require gradient.\n",
      "\n",
      "This method also affects forward mode AD gradients and the result will never\n",
      "have forward mode AD gradients.\n",
      "\n",
      ".. note::\n",
      "\n",
      "  Returned Tensor shares the same storage with the original one.\n",
      "  In-place modifications on either of them will be seen, and may trigger\n",
      "  errors in correctness checks.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "w.detach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
